# Fine-tuning using sampled sets to evaluate the impact of dataset size
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/samples/100/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/samples/100/val.jsonl --output_dir ./nllb_finetuned_samples/nllb_finetuned_base_100 --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/samples/200/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/samples/200/val.jsonl --output_dir ./nllb_finetuned_samples/nllb_finetuned_base_200 --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/samples/300/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/samples/300/val.jsonl --output_dir ./nllb_finetuned_samples/nllb_finetuned_base_300 --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/samples/400/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/samples/400/val.jsonl --output_dir ./nllb_finetuned_samples/nllb_finetuned_base_400 --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/samples/500/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/samples/500/val.jsonl --output_dir ./nllb_finetuned_samples/nllb_finetuned_base_500 --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/samples/600/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/samples/600/val.jsonl --output_dir ./nllb_finetuned_samples/nllb_finetuned_base_600 --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/samples/700/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/samples/700/val.jsonl --output_dir ./nllb_finetuned_samples/nllb_finetuned_base_700 --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/samples/800/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/samples/800/val.jsonl --output_dir ./nllb_finetuned_samples/nllb_finetuned_base_800 --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/samples/900/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/samples/900/val.jsonl --output_dir ./nllb_finetuned_samples/nllb_finetuned_base_900 --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/samples/1000/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/samples/1000/val.jsonl --output_dir ./nllb_finetuned_samples/nllb_finetuned_base_1000 --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard

# Ablation
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-bqi/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-bqi/val.jsonl --output_dir ./nllb-finetuned_ablation/nllb_finetuned_base_bqi --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-lki/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-lki/val.jsonl --output_dir ./nllb-finetuned_ablation/nllb_finetuned_base_lki --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-mzn/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-mzn/val.jsonl --output_dir ./nllb-finetuned_ablation/nllb_finetuned_base_mzn --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-hac/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-hac/val.jsonl --output_dir ./nllb-finetuned_ablation/nllb_finetuned_base_hac --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-glk/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-glk/val.jsonl --output_dir ./nllb-finetuned_ablation/nllb_finetuned_base_glk --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-zza/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-zza/val.jsonl --output_dir ./nllb-finetuned_ablation/nllb_finetuned_base_zza --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-sdh/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-sdh/val.jsonl --output_dir ./nllb-finetuned_ablation/nllb_finetuned_base_sdh --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard
python run_translation.py --model_name_or_path ./nllb_extended --do_train --do_eval --train_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-tly/train.jsonl --validation_file /home/user/ahmadi/DOLMA/fine-tune/ablation/1000-tly/val.jsonl --output_dir ./nllb-finetuned_ablation/nllb_finetuned_base_tly --per_device_train_batch_size 16 --learning_rate 5e-4 --num_train_epochs 20 --warmup_ratio 0.15 --fp16 --predict_with_generate --logging_dir ./runs --logging_strategy steps --logging_steps 100 --logging_first_step --evaluation_strategy epoch --save_strategy epoch --save_total_limit 2 --metric_for_best_model eval_bleu --load_best_model_at_end --greater_is_better true --max_source_length 128 --max_target_length 128 --pad_to_max_length --num_beams 5 --weight_decay 0.01 --seed 42 --overwrite_output_dir --report_to tensorboard